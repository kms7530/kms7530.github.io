---
title: Optimazation
date: 2023-03-20T13:34:00Z
draft: false
katex: true
---

# Generalization

- 학습을 하고 테스트를 하게 되면 각각의 error률이 나오게 된다.
- 이때, train data와 test data의 error률간의 격차를 generalization gap 이라 한다.
- 우리는 이를 줄이는 것에 목적을 둔다.

## Cross-validation

![스크린샷%202023-03-20%20오후%201.23.30.png](/Optimazation%20cc410498f32f4fa0a019661dc259f885/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-03-20_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.23.30.png)

- 전통적인 방식에서는 데이터가 주어졌을 때 Test data와 Train data를 고정된 구간으로 나누었다.
- 하지만 여기서 train data를 k개로 나누어 1구간을 돌아가며 validation data로 지정한다.
- 이를 cross-validation 이라 한다.

## Bias and Variance

- Bias: 편향, 쉽게 말해서 정확성을 말하는 것이다. 낮을수록 정확하다는 것이다.
- Variance: 분산, 쉽게말해 탄착군 형성을 말하는 것이다. 낮을수록 많이 군집화 되어있다는 것이다.

### Trade-off

- 우리는 위의 두 요소와 노이즈를 제거하는데 목표를 두고있다.
- 하지만, 세 개의 모든 요소를 줄이는 것은 불가능하다. 뭐 하나를 얻으면 잃는게 있는 방식이다.
- 수식은 아래와 같다.

$$
E[(t-\hat f)^2] = E[(f-E[\hat f]^2)^2] + e[(E[\hat f]-\hat f)^2]+E[\epsilon]
$$

- 위의 우변의 각 항은 bias, variance 그리고 noise이다.

## Bootstrapping

- 데이터가 주어졌을 때 모든 데이터를 사용하지 않고, 일부 갯수를 random하게 뽑아 각각의 모델을 학습시키는 것 이다.
- 이를 이용한 것이 Bagging이다.

## Bagging vs Boosting

![Untitled](/Optimazation%20cc410498f32f4fa0a019661dc259f885/Untitled.png)

- Boosting: 위의 부트스트렙을 이용하여 각각의 모델을 만들어 진행한다.
- Boosting: training 데이터에서 어려워 하는 데이터만 모아 모델을 따로 만들어 직렬로 연결한 것 이다.
