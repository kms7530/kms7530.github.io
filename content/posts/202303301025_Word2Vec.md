---
title: Word2Vec
date: 2023-03-30T10:25:00Z
draft: false
katex: true
---

# Word Embedding

- 단어를 벡터로 변환하여 단어를 최적의 좌표에 mapping 시키는 것을 말한다.
- 여기서 최적의 좌표란 연관성이 있는 단어는 군집화 시키는 것을 말한다.

# Word2Vec

- 같은 문장에서 나타나는 인접 단어는 단어의 의미가 유사할것 이라는 생각에서 시작된 기술이다.
- 고양이를 예시로 들게 되면 $P(W|cat)$ 이 높은 것을 뽑게 되었을 때 문장에서 학습한것을 기반으로 다음 단어로 가장 많이 나오는 것을 가져오게 된다.

## 작동 원리

- 문장 “The fat cat sat on the mat”이 있다 가정하자.
- 여기서 단어별로 나누게 되면 {The, fat, cat, sa,t on, the, mat}가 된다.
- 여기서 CNN의 channel과 같이 sliding window처럼 각 단어별로 넘어가며 학습을 한다.
    - 아래는 다른 문장의 예시인데, 도움을 줄 수 있을것 같아 가져왔다.
        
![Untitled](/Word2Vec%209755b1c721844486a3ef5ec10e4865cf/Untitled.png)
        
- 위와 같이 가져온 단어들은 아래와 같은 모델에 넣어 학습시킨다.
    
![Untitled](/Word2Vec%209755b1c721844486a3ef5ec10e4865cf/Untitled%201.png)
    
- 여기서 Projection layer의 경우 hidden layer라고 보면 된다.
- 해당 [웹사이트](https://ronxin.github.io/wevi/)를 참고해보는것도 좋을 것 같다.

## 얻을수 있는 점

- 단어간의 상관관계가 벡터로 인해 생기게 된다.
- 가령 [Queen] - [Woman] + [Man] = [King]과 같은 연산이 가능해지면서 단어간의 관계를 알아볼수 있다.
- [word2vec.kr](https://word2vec.kr/)에서 실험해 볼수 있다.

## 응용분야

- 단어 유사도 측정, 기계 번역, 감정 분석 등에 사용될수 있다.
