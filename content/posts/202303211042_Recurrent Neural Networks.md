---
title: Recurrent Neural Networks
date: 2023-03-21T10:42:00Z
draft: false
katex: true
---

# Sequential Data와 기본 Model

- 시퀀셜 데이터의 경우 언제 입력값이 끝나게 될지 모르기 때문에 이에 맞는 Model이 있어야 한다.
- 그래서 가장 간단한 모델로 입력값을 받아 다음 입력값을 예측하는 모델을 생각해 볼 수 있다.
    - 이때 과거의 정보를 축적해 가져가는 경우 너무 많은 정보 량이 있기 때문에, 정해진 구간만큼만 가져가게 된다.

## Markov Model

- 위의 예시에서 가장 간단히 표현 가능한 모델이다. 아래는 이에 대한 식이다.

$$
p(x_1,...,x_T)=p(x_T|x_{T-1})p(x_{T-1}|x_{t-2})...p(x_2|x_1)p(x_1)=\Pi^T_{t=1}p(x_t|x_{t-1})
$$

- 이 모델의 한계로는 이전의 많은 정보에 대해 모두 고려하기가 어렵다는 점이다.

## Latent Autoregressive Model

- 위의 단점을 hidden state에 과거의 정보를 요약해 다음 상태에 전달해 줌으로써 해결한 모델이다.

# RNN

- 위의 Latent Autoregressive Model을 활용해 만든것이 RNN이다.
- 하지만, 여기에 큰 단점이 있는데 바로 정보를 요약하여 계속 넘겨주게 되면 이후 먼 곳의 레이어에서는 정보(기울기)가 소실되거나 증폭되어 학습되지 않게 된다.

# LSTM

![스크린샷%202023-03-21%20오전%2011.01.51.png](/Recurrent%20Neural%20Networks%20d7cf7f88774b48a6827088724975ed96/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-03-21_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.01.51.png)

- 위의 단점을 해결한 모델이 LSTM이다.
    - 여기서 핵심은 gate를 이용하여, 어떠한 정보를 가져가고 어떠한 정보를 소실할지 정하게 되는것이다.
        - Forget Gate의 경우 이전 정보에 대해 어떤 정보를 버릴지 선별한다.
        - Input Gate의 경우 이전 정보와 현재의 정보를 받아 어떤 정보를 가져갈지 선별한다.
        - Update Gate의 경우 Input Gate와 Forget Gate에서 받은 정보를 이용해 정보를 갱신한다.
        - Output Gate의 경우 다음 레이어에 전달할 정보를 보내준다.
- 위와 같은 유닛을 input data의 길이만큼 이어붙이게 되면 그 크기만큼 output이 나오게 된다.
