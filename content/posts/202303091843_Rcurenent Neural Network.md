---
title: Rcurenent Neural Network
date: 2023-03-09T18:43:00Z
draft: false
katex: true
---

# 수업 방향

시계열 데이터를 처리하기 위한 Vanilla RNN을 알아보고 이의 한계점을 알아보자. 

# 수업 내용

## RNN의 존재 이유

- 시계열 데이터의 경우 데이터의 크기가 정해지지 않음. 물론 다른 데이터 타입(이미지 등)과 비교 했을 때 말이다.
- 이를 해결하기 위해 연속적인 데이터를 처리하기 위해 생긴 것이다.

## RNN의 수학적인 접근 - 기초

- 먼저 데이터의 순서나 과거 정보의 수정이 생기는 경우 데이터의 확률분포가 바뀜을 알아야 한다.
- 수식으로 보게되면 다음과 같다.
    - $\Pi^t_{s=1}=P(X_s|X_{s-1},...,X_1)$그
- 그러나 위와 같은 식으로는 가변적인 데이터를 다룰수 없는 상황이다.
    - 그러한 이유로 정해진 길이($\tau$) 만큼을 정해 하나의 뭉텅이로 만든다.
    - 물론 이 길이를 정하는 것은 domain 지식을 이용해야 한다.
- 또 하나의 문제점이 위의 방식은 모든 입력에 대해 parameter로 가져가야 한다.
    - 이를 해결하기 위해 관찰 직전의 데이터를 제외한 데이터들을 모두 *“요약”*하여 가지고 있는다.
    - 이를 수식으로 해결하면 다음과 같다.
        - $H_t=Net_\theta(H_{t-1},X_{t-1})\rightarrow X_t~P(X_t|X_{t-1},H_t)$
        - $H_t$는 연쇄적으로 이전 값들을 포함한다.

## RNN의 수학적인 접근 - 본편

- 기본적인 RNN의 수식은 다음과 같다. 단, 과거의 정보를 다룰수 없는 모델이다.
    - $O_t=H_tW^{(2)}+b^{(2)}$
    - $H_t=\sigma(X_tW^{(1)}+b^{(1)})$
- 위의 식을 잠재변수(과거 정보)를 포함하는 식으로 바꾸면 다음과 같다.
    - $O_t=H_tW^{(2)}+b^{(2)}$
    - $H_t=\sigma(X_tW^{(1)}+H_{t-1}W^{(1)}_H+b^{(1)})$
    - 위의 $H_t$는 연쇠적으로 위의 식을 따라가게 된다.
- RNN에서는 역전파를 BPTT(Backpropagation Through Time)이라 하는데 아래의 사항에 대해 알아두자.
    - 단, 이곳에서 알고가야 하는 점은 시퀀스가 길어질수록 $\Pi$ 가 들어간 항에서 불안정해진다는 것이다.
    - 불안정이란, 기울기 소실 즉 기울기가 0이되어 더이상 학습이 힘들어 지는것을 말한다.
    - 이를 해결하기 위해 truncated BPTT를 사용한다.
        - 일정 길이에서 역전파를 끊어버리는 것을 말한다.
    - 하지만 위의 해결도 근본적인 해결방안은 될수 없다. 따라서 LSTM 혹은 GRU와 같은 RNN Network를 사용해야한다.
