---
title: Gradient descent - 2/2
date: 2023-03-08T14:10:00Z
draft: false
katex: true
---

# Moore-Penrose 넘어로

- 이전의 선형회귀에서는 무어-펜로즈 방식을 사용하였다.
    - 그러나 이를 이용하게 되면 “선형”회귀에서만 사용이 가능해져, 조금 더 universal 한 방법으로 바꾸어 보려 한다.
- 이를 위해 이번에는 목적식으로 $||y-X\beta ||_2$ 이며 이를 최소화 하는 값 $\beta$ 를 찾는것이다. 이를 찾기 위한 식은 다음과 같다.

$$
\nabla_ \beta ||y-X \beta ||_2 = (\delta _{\beta} || y-X \beta ||_2, ... , \delta _{\beta_d}|| y-X \beta || _2 )
$$

$$
\to \delta_{\beta_k}||y-X\beta||2=\delta_{\beta_k} \{\frac1n \sum^n_{i=1}(y_i-\sum^d_{j=1}X_{ij}\beta_j)^2\}^{\frac12} \newline
=-\frac{X^T_k(y-X\beta)}{n||y-X\beta||_2}
$$

- 위의 복잡한 식을 개념적으로 풀어 이해하면 밑에와 같다.

$$
\beta^{(t+1)} \leftarrow \beta^{(t)}-\lambda \nabla_\beta ||y-X\beta^t||
$$

- 위의 식은 다음 베타의 값이 이전의 베타에서 Learning-rate인 람다 에 기울기를 곱한 값 이라는 뜻이다.
- 위의 식을 조금 더 정리하여 행렬 친화적으로 변형하면 다음과 같다.

$$
\beta^{(t+1)} \leftarrow \beta^{(t)}-\frac{2\lambda}n X^T ||y-X\beta^t||
$$

# 경사하강법의 코드화

```python
# 이전에는 임계값을 정하여 도달하게 되는 경우 멈추었지만, 
# 해당 방법 그리고 최근의 머신/딥러닝에서는 학습 step을
# 정하여, 정해진 횟수만큼 학습을 한 후 종료한다. 
for i in range(step):
	err = y - x @ beta
	grad = - X.T @ err
	beta = beta - learning_rate * grad
```

# 하지만…

- 경사하강법이 만능으로 쓰이는 것은 아니다. 이유로 몇가지가 있다.
    - 먼저, 어디까지나 “적절한” 학습률과 학습횟수를 선택했을때 수렴이 보장된다.
    - 그리고 비선형최귀 문제의 경우 수렴이 항상 보장되지는 않는다.

# 문제 해결 - SGD

- 바로 Stochastic Gradient Descent 즉 SGD 이다.
- 이는 풀어 말하면 확률적 경사 하강법으로 의미 그대로 모든 데이터가 아닌 일부 데이터(batch)를 선택해 업데이트 하는것이다.
- 이를 수식으로 쓰면 다음과 같다. 아래의 식에서 b는 batch 크기를 의미한다.

$$
\beta^{(t+1)}\leftarrow \beta^t+\frac{2\lambda}bX^T_b(y_b-X_b\beta^t)
$$

- 이렇게 매번 다른 batch를 이용하여 업데이트 하게되면 다음의 그래프와 같이 업데이트 값이 불규칙하게 나온다.
    
![스크린샷 2023-03-08 오후 2.40.16.png](/Gradient%20descent%20-%202%202%208f4389f8621649ffad4001e46edabf2f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.40.16.png)
    
- 이러한 방식으로 업데이트가 되다보니, 이전의 자료에서 확인한것과 다르게 top-view에서 보면 학습 진행경로가 불규칙하다.
    
![스크린샷 2023-03-08 오후 2.41.15.png](/Gradient%20descent%20-%202%202%208f4389f8621649ffad4001e46edabf2f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.41.15.png)
    
- SGD는 이전의 경사하강법과 다르게, 볼록이 아닌 목적식에서 작동할 뿐만 아니라, 학습 시간도 훨씬 덜 걸리게 된다.
    
![스크린샷 2023-03-08 오후 2.42.15.png](/Gradient%20descent%20-%202%202%208f4389f8621649ffad4001e46edabf2f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.42.15.png)
    
- 또한 GPU의 메모리가 한정되어 있기 때문에 batch 사이즈로 데이터를 나누어주는것은 필수적이다.
