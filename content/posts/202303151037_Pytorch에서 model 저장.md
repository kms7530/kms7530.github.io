---
title: Pytorch에서 model 저장
date: 2023-03-15T10:37:00Z
draft: false
katex: true
---

# model.save

- 학습된 모델의 결과를 저장하는 함수다.
- 모델 학습 중간마다 혹은 최종 결과를 저장해 외부 연구자와 공유하여 재연성을 향상한다.
- 모델 저장에는 두 가지 방법이 있다.
    - 모델 내의 파라미터를 가져와 저장하기.
    - 모델 아키텍쳐 자체를 저장하기.

# torchsummary.summary

- keras에서 나오게되는 모델 요약 표시 방법을 pytorch에서 구현해놓은 것이다.
- 이미 구현된 model에 대해 구조를 표시하면 다음과 같다.

```python
summary(model, (1, 28, 28))
# ----------------------------------------------------------------
#         Layer (type)               Output Shape         Param #
# ================================================================
#             Conv2d-1           [-1, 10, 24, 24]             260
#             Conv2d-2             [-1, 20, 8, 8]           5,020
#          Dropout2d-3             [-1, 20, 8, 8]               0
#             Linear-4                   [-1, 50]          16,050
#             Linear-5                   [-1, 10]             510
# ================================================================
# Total params: 21,840
# Trainable params: 21,840
# Non-trainable params: 0
# ----------------------------------------------------------------
# Input size (MB): 0.00
# Forward/backward pass size (MB): 0.06
# Params size (MB): 0.08
# Estimated Total Size (MB): 0.15
# ----------------------------------------------------------------
```

# checkpoints

- 제목 그대로 학습 중간마다 결과를 확인하여 저장하는 방법이다.
- Early stopping 기법과 같은 내용으로 loss와 metric 값을 지속적으로 확인하여 값을 저장한다.
- 저장시 epoch, loss, metric을 함께 저장하여 사용한다.

# Transfer Learning

- 대용량 dataset으로 제작된 model에 대해 현재 dataset을 적용시키는 것이다.
- 현재 DL에서 많이 사용되는 방식이다.
- Backbone architecture가 잘 학습된 모델에서 일부만 변경하여 학습을 수행한다.

## Freezing

- Pre-trained model에서 각 레이어별로 나누었을 때, 어떠한 레이어를 기준으로 back-propergation을 막아두고 학습을 시키는 방법이다.
- 위에서 언급한 trasfer learning에서 사용되는 방법이다.
