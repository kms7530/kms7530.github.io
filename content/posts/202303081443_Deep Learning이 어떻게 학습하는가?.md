---
title: Deep Learning이 어떻게 학습하는가?
date: 2023-03-08T14:43:00Z
draft: false
katex: true
---

## 신경망의 수식화

- $O=XW+b \rightarrow$ O의 경우 output, x의 경우 input data, w의 경우 wight 그리고 b의 경우 bais를 나타낸다.

### Softmax

- 위의 수식에서 O 값을 특정 k 클래스에 속할 확률로 변환을 원하는 경우 softmax를 이용하게 된다.
- $softmax(o)=(\frac {exp(o_1)}{\Sigma^p_{k=1}exp(o_k)},...,\frac {exp(o_p)}{\Sigma^p_{k=1}exp(o_k)})$
- 위의 함수를 통과하게 되면, 각 클레스에 대해 확률로 나오게 된다.

### One hot encoding

- 추론을 원하게 되는 경우 출력된 O 값을 one-hot encoding으로 넣게 된다.

### 둘간의 차이(Softmax와 one-hot encoding)

- 학습시에는 softmax를 이용하여 오차값을 계산하기 위해 클레스 별 확률을 구한다.
- 추론시에는 one-hot encoding을 이용하여 어떠한 클레스인지 구분만 한다.

## 활성함수

- 신경망은 활성함수가 없게되면 일반 선형모델들과 다를바가 없다.
- 이를 해결하기 위해 activation function을 사용하는 것이다.
- sigmoid와 tanh 함수가 전통적으로 많이 쓰였으나 최근은 ReLU 함수를 사용하고 있다.
- $\sigma(x)=\frac{1}{1+e^{-x}},\ tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}},\ ReLU(x)=max\{0,x\}$
- 위의 함수를 레이어와 레이어 사이에 넣게 된다.

## Layer를 쌓는 이유

- 층이 깊고 입력 param이 적은것과 층이 낮고 입력 param이 많은것을 비교해보자.
    - 후자를 사용하게 되면 내부에 필요하게 되는 뉴런 갯수가 기하급수로 늘어나게 된다.
    - 그러나 전자를 사용하게 되면 필요 뉴런 갯수가 줄어들어 계산시 시간이 줄어든다.

## Backpropagation

- 역전파 알고리즘은 chain-rule(연쇄법칙)에 기반한 auto-differentiation(자동미분)을 이용하게 된다.
- 예시는 다음과 같다.
    - $z=(x+y)^2$
    - $\rightarrow\ z=w^2\ \rightarrow\ \frac{\partial z}{\partial w}=2w$
    - $\rightarrow\ w=x+y\ \rightarrow\ \frac{\partial w}{\partial x}=1, \ \frac{\partial w}{\partial y}=1$
    - $\therefore \frac{\partial z}{\partial x}=\frac{\partial z}{\partial w}\frac{\partial w}{\partial x}=2w\times1=2(x+y)$
