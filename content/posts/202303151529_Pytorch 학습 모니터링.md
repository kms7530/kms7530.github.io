---
title: Pytorch 학습 모니터링
date: 2023-03-15T15:29:00Z
draft: false
katex: true
---

# 학습 진행 모니터링

- 대부분 학습을 진행할 때 콘솔의 print 된 정보를 가지고 어느정도 학습이 됐는지 확인한다.
    - 이 방법은 여러 단점이 있는데, 무엇보다도 따로 저장하지 않으면 후에 확인이 어렵다는 점과 다른 모델과 성능을 비교하기에 쉽지 않다는 점이 있다.
- 이러한 이유로 인해 Tensorboard 혹은 weight & biases 등 API를 사용하게 된다.

# Tensorboard

- 해당 API는 TensorFlow에서 시작된 시각화 도구다.
    - 물론 Pytorch에서도 사용이 가능하다.
- 이는 따로 가입과 같은 단계가 필요없으며 torch가 설치되어있다면 바로 사용 가능하다.
- Tensorboard에서 저장 가능한 데이터는 다음과 같다.
    - scalar: metric 등 연속되는 상수값을 표시
    - graph: 모델의 computational Graph 표시
    - histogram: weight 등 값의 분포 표시
    - image: 예측 값과 실제 값을 비교 표시
    - mesh: 3D 형태의 데이터를 표시.
- 예시는 다음과 같다.

```python
import os

# 로그 저장용 directory 생성. 
logs_base_dir = "logs"
os.makedirs(logs_base_dir, exist_ok=True)

# Tensorboard사용을 위해 API 가져오기. 
from torch.utils.tensorboard import SummaryWriter
import numpy as np

# 렌덤 값 저장. 
writer = SummaryWriter(logs_base_dir)
    for n_iter in range(100):
        writer.add_scalar('Loss/train', np.random.random(), n_iter) 
        writer.add_scalar('Loss/test', np.random.random(), n_iter) 
        writer.add_scalar('Accuracy/train', np.random.random(), n_iter) 
        writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
# 파일 쓰기. 
writer.flush()

# 노트북에서 바로보기 위한 명령어. 
%load_ext tensorboard
%tensorboard --logdir {logs_base_dir}
```

# weight & biases

- 위의 Tensorboard와 다르게, 상용툴로 기본 기능은 무료지만 일부 기능부터는 유료인 서비스이다.
- 협업, code versioning 등 추가적인 기능을 제공한다.
- 서비스 이용을 위해 가입이 필요하고, 가입 후 API 키를 발급받아야 사용이 가능하다.
- colab 등에서 기본적으로 제공하지 않으므로, pip를 통해 설치가 필요하다.
- 먼저, 아래의 명령어를 이용해 설치한다.

```bash
pip install wandb -q
```

- 아래의 코드는  사용 방법에 대한 예시다.

```python
# 기본적으로 저장할 값에 대해 설정. 
config = {"epochs": EPOCHS, 
        "batch_size": BATCH_SIZE, 
        "learning_rate" : LEARNING_RATE} 
# wandb에 어떠한 값을 어디에 저장할지 입력. 
# 해당 동작 수행 시 API 키에 대해 물어봄. 
wandb.init(project="my-test-project", config=config)

# 아래와 같이 값 설정 가능. 
wandb.config.batch_size = BATCH_SIZE
wandb.config.learning_rate = LEARNING_RATE

# 값 저장. 
for e in range(1, EPOCHS+1):
    epoch_loss = 0
    epoch_acc = 0
    for X_batch, y_batch in train_dataset:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device).type(torch.cuda.FloatTensor) #...
    optimizer.step()

wandb.log({'accuracy': train_acc, 'loss': train_loss})
```
